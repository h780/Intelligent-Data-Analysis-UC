{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Exercise for  FPM Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploring properties of the dataset accidents_10k.dat. Read more about it here:  http://fimi.uantwerpen.be/data/accidents.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \r\n",
      "2 5 7 8 9 10 12 13 14 15 16 17 18 20 22 23 24 25 27 28 29 32 33 34 35 36 37 38 39 \r\n",
      "7 10 12 13 14 15 16 17 18 20 25 28 29 30 33 40 41 42 43 44 45 46 47 48 49 50 51 52 \r\n",
      "1 5 8 10 12 14 15 16 17 18 19 20 21 22 24 25 26 27 28 29 30 31 41 43 46 48 49 51 52 53 54 55 56 57 58 59 60 61 \r\n",
      "5 8 10 12 14 15 16 17 18 21 22 24 25 26 27 28 29 31 33 36 38 39 41 43 46 56 62 63 64 65 66 67 68 \r\n",
      "7 8 10 12 17 18 21 23 24 26 27 28 29 30 33 34 35 36 38 41 43 47 59 63 66 69 70 71 72 73 74 75 76 77 78 79 \r\n",
      "1 12 14 15 16 17 18 21 22 23 24 25 27 28 29 30 31 35 38 41 43 44 53 56 57 58 59 60 63 66 80 81 82 83 84 \r\n",
      "10 12 14 15 16 17 18 21 22 24 25 26 27 28 29 30 31 33 39 41 43 44 46 49 59 60 62 63 66 82 \r\n",
      "1 8 10 12 14 15 16 17 18 21 22 23 24 25 27 29 30 31 38 41 43 53 56 59 61 63 66 68 85 86 87 88 89 \r\n",
      "1 8 12 13 14 15 16 17 18 22 24 25 28 30 38 41 42 43 46 49 60 63 64 66 80 82 84 90 91 92 93 94 95 \r\n"
     ]
    }
   ],
   "source": [
    "!head accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1a:** </span>. How many items are there in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\r\n"
     ]
    }
   ],
   "source": [
    "!awk -- '{for (i = 1; i <= NF; i++) wc[$i] += 1}; END {print length(wc)}' accidents_10k.dat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Number of items in the data file accidents_10k are 310 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1b:** </span> How many transactions are present in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 accidents_10k.dat\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Number of transactions in the data file accidents_10k are 10000. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1c:** </span>.  What is the length of the smallest transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 23\r\n"
     ]
    }
   ],
   "source": [
    "!awk -- 'NR==1 || NF<lw {lw=NF} END {print \"Length: \" lw}' accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:**Length of smallest transaction in the data file accidents_10k.dat is 23 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1d:** </span>  What is the length of the longest transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 45\r\n"
     ]
    }
   ],
   "source": [
    "!awk -- 'NR==1 || NF>len {len=NF; line=$0} END {print \"Length: \" len}' accidents_10k.dat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:**Length of longest transaction in the data file accidents_10k.dat is 45</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1e:** </span>  What is the size of the search space of frequent itemsets in this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size Space Size 2.085925e+93"
     ]
    }
   ],
   "source": [
    "!awk -- '{for (i=1;i<=NF;i++) WC[$i]+=1}; END {printf \"Size Space Size %e\",2^length(WC)}' accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** The size of search space for frequent itemsets is 2.085925e+93</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1f:** </span> \n",
    "Assume that you work for the deparment of transportation that collected this data. What benefit do you see in using itemset mining approaches on this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** As the data contains thousands of transactions and multiple transactions have multiple items in it, so using the Itemset Mining will help us to prune the subsets from data which are the most important and will allow us to find different relationships between the data. Also, with the help of support for each itemsets, we can discover the most important cause for the accidents and then develop more robut system to reduce the number of accidents due to various factors. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1g:** </span>  What type of itemsets (frequent, maximial or closed) would you be interested in discovering this dataset? State your reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** 1. Closed itemset is the best choice for this dataset as it takes less time to generate closed itemset than the frequent itemsets, on other hand it will generate loss-less summary of the itemsets with the support also which is not available in maximal itemsets.  2. Frequent itemset can be second best choice as we will get the most frequent itemsets; we can generte the required association rules with the number of frequent itemsets.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1h:** </span>  What minsup threshold would you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** We can assume minsup threshold of 50-70 %, we can also go below 50 % and check whether we can find the occurance of more frequent itemsets. But, as we need to prune the most important itemsets from the data, so we should go ahead with minimum support which is between 50-70%, it will prune the not required data and also provide the important causes of the accidents. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating frequent, maximal and closed itemsets using $\\color{red}{\\text{Apriori}}$, $\\color{red}{\\text{ECLAT}}$, and $\\color{red}{\\text{FPGrowth}}$ algorihtms from the dataset accidents_10k.dat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2a:** </span> Generate frequent itemsets using Apriori, for minsup = 2000, 3000, and 4000. Which of these minsup thresholds results in a maximum number of frequent itemsets? Which of these minsup thresholds results in a least number of frequent itemsets? Provide a rationale for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!chmod u+x apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ./apriori [options] infile [outfile]\r\n",
      "find frequent item sets with the apriori algorithm\r\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\r\n",
      "-t#      target type                              (default: s)\r\n",
      "         (s: frequent, c: closed, m: maximal item sets,\r\n",
      "          g: generators, r: association rules)\r\n",
      "-m#      minimum number of items per set/rule     (default: 1)\r\n",
      "-n#      maximum number of items per set/rule     (default: no limit)\r\n",
      "-s#      minimum support of an item set/rule      (default: 10%)\r\n",
      "-S#      maximum support of an item set/rule      (default: 100%)\r\n",
      "         (positive: percentage, negative: absolute number)\r\n",
      "-o       use original rule support definition     (body & head)\r\n",
      "-c#      minimum confidence of an assoc. rule     (default: 80%)\r\n",
      "-e#      additional evaluation measure            (default: none)\r\n",
      "-a#      aggregation mode for evaluation measure  (default: none)\r\n",
      "-d#      threshold for add. evaluation measure    (default: 10%)\r\n",
      "-i       invalidate eval. below expected support  (default: evaluate all)\r\n",
      "-p#      (min. size for) pruning with evaluation  (default: no pruning)\r\n",
      "         (< 0: weak forward, > 0 strong forward, = 0: backward pruning)\r\n",
      "-q#      sort items w.r.t. their frequency        (default: 2)\r\n",
      "         (1: ascending, -1: descending, 0: do not sort,\r\n",
      "          2: ascending, -2: descending w.r.t. transaction size sum)\r\n",
      "-u#      filter unused items from transactions    (default: 0.01)\r\n",
      "         (0: do not filter items w.r.t. usage in sets,\r\n",
      "         <0: fraction of removed items for filtering,\r\n",
      "         >0: take execution times ratio into account)\r\n",
      "-x       do not prune with perfect extensions     (default: prune)\r\n",
      "-y       a-posteriori pruning of infrequent item sets\r\n",
      "-T       do not organize transactions as a prefix tree\r\n",
      "-F#:#..  support border for filtering item sets   (default: none)\r\n",
      "         (list of minimum support values, one per item set size,\r\n",
      "         starting at the minimum size, as given with option -m#)\r\n",
      "-R#      read item selection/appearance indicators\r\n",
      "-P#      write a pattern spectrum to a file\r\n",
      "-Z       print item set statistics (number of item sets per size)\r\n",
      "-N       do not pre-format some integer numbers   (default: do)\r\n",
      "-g       write item names in scanable form (quote certain characters)\r\n",
      "-h#      record header  for output                (default: \"\")\r\n",
      "-k#      item separator for output                (default: \" \")\r\n",
      "-I#      implication sign for association rules   (default: \" <- \")\r\n",
      "-v#      output format for set/rule information   (default: \" (%S)\")\r\n",
      "-j#      sort item sets in output by their size   (default: no sorting)\r\n",
      "         (< 0: descending, > 0: ascending order)\r\n",
      "-w       integer transaction weight in last field (default: only items)\r\n",
      "-r#      record/transaction separators            (default: \"\\n\")\r\n",
      "-f#      field /item        separators            (default: \" \\t,\")\r\n",
      "-b#      blank   characters                       (default: \" \\t\\r\")\r\n",
      "-C#      comment characters                       (default: \"#\")\r\n",
      "-!       print additional option information\r\n",
      "infile   file to read transactions from           [required]\r\n",
      "outfile  file to write item sets/assoc. rules to  [optional]\r\n"
     ]
    }
   ],
   "source": [
    "!./apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [20250 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [17.36s].\n",
      "writing A_AP_Freq_minsup2000.txt ... [851034 set(s)] done [0.10s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-2000 accidents_10k.dat A_AP_Freq_minsup2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851034 A_AP_Freq_minsup2000.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l A_AP_Freq_minsup2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [38 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9674/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [24741 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 done [4.02s].\n",
      "writing A_AP_Freq_minsup3000.txt ... [133799 set(s)] done [0.02s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-3000 accidents_10k.dat A_AP_Freq_minsup3000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133799 A_AP_Freq_minsup3000.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l A_AP_Freq_minsup3000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [22267 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [1.18s].\n",
      "writing A_AP_Freq_minsup4000.txt ... [29501 set(s)] done [0.00s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-4000 accidents_10k.dat A_AP_Freq_minsup4000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29501 A_AP_Freq_minsup4000.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l A_AP_Freq_minsup4000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Firstly, the search space which we have is of 2 raised to power 310. This is a huge space for data analysis. We look for data which is more frequent using the mininum support values. If the minimum support is on lower scale, then it will capture high amount of frequent itemsets and if it is on higher side, then it will capture less number of frequent itemsets. Hence, itemsets with minsup as 2000 generates the highest number of frequent itemsets i.e. 851034 and itemsets with minsup as 4000 generates the lowest number of frequent itemsets i.e. 29501. Apriori prunes superset for that</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2b:** </span>   Using Apriori, compare the execution time for finding frequent itemsets for minsup = 2000, 3000, and 4000. Which of these minsup thresholds takes the least amount of time? Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [17.12s].\n",
      "writing A_AP_Freq_minsup2001.txt ... [851034 set(s)] done [0.10s].\n",
      "17 secs  690608 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-2000 accidents_10k.dat A_AP_Freq_minsup2001.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [38 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9674/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [24741 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 done [3.93s].\n",
      "writing A_AP_Freq_minsup3001.txt ... [133799 set(s)] done [0.02s].\n",
      "4 secs  247507 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-3000 accidents_10k.dat A_AP_Freq_minsup3001.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [22267 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [1.17s].\n",
      "writing A_AP_Freq_minsup4001.txt ... [29501 set(s)] done [0.01s].\n",
      "1 secs  431825 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-4000 accidents_10k.dat A_AP_Freq_minsup4001.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Minsup with value 4000 takes the least amount of time i.e. 1 secs 431825 microsecs, as it will prune the itemsets which are below the support of 4000 at each level(k), which in turn will allow the tree to reach the final leaf node more efficiently.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2c:** </span> Using Apriori, find the frequent itemsets for minsup = 2000, 3000, and 4000. Determine the number of itemsets for each size (1 to max length of an itemset). What trends do you see that are common for all three minsup thresholds? What trends do you see that are different? Provide a rationale for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     49 1\n",
      "    705 2\n",
      "   5285 3\n",
      "  23745 4\n",
      "  69647 5\n",
      " 139628 6\n",
      " 195730 7\n",
      " 193299 8\n",
      " 133819 9\n",
      "  63937 10\n",
      "  20497 11\n",
      "   4189 12\n",
      "    483 13\n",
      "     21 14\n"
     ]
    }
   ],
   "source": [
    "!awk '{print NF-1}' A_AP_Freq_minsup2000.txt|sort -n|uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     38 1\n",
      "    468 2\n",
      "   2830 3\n",
      "   9887 4\n",
      "  21779 5\n",
      "  31964 6\n",
      "  32020 7\n",
      "  21862 8\n",
      "   9839 9\n",
      "   2705 10\n",
      "    387 11\n",
      "     20 12\n"
     ]
    }
   ],
   "source": [
    "!awk '{print NF-1}' A_AP_Freq_minsup3000.txt|sort -n|uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     33 1\r\n",
      "    319 2\r\n",
      "   1492 3\r\n",
      "   4043 4\r\n",
      "   6926 5\r\n",
      "   7751 6\r\n",
      "   5626 7\r\n",
      "   2546 8\r\n",
      "    668 9\r\n",
      "     91 10\r\n",
      "      6 11\r\n"
     ]
    }
   ],
   "source": [
    "!awk '{print NF-1}' A_AP_Freq_minsup4000.txt|sort -n|uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Itemsets with minsup as 2000 are ranging from length 1 to 14. minsup=2000 captures the highest number because the support is on lower side, whereas minsup=3000 and 4000 have less number of lengths of itemsets when compared with minsup=2000 due to obvious reasons that support is high for both of them respectively. Also, the number of itemsets is increasing for first few lengths, but then it is decreasing moving ahead because there may not be large number of frequent itemsets which are of larger lengths.</span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2d:** </span>  Using Apriori with minsup=2000, compare the number of frequent, maximal, and closed itemsets. Which is the largest set and which is the smallest set? Provide a rationale for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.03s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [17.51s].\n",
      "writing A_AP_Freq_minsup2000.txt ... [851034 set(s)] done [0.11s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-2000 accidents_10k.dat A_AP_Freq_minsup2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 done [35.14s].\n",
      "filtering for maximal item sets ... done [0.05s].\n",
      "writing A_AP_Max_minsup2000.txt ... [12330 set(s)] done [0.01s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -tm -s-2000 accidents_10k.dat A_AP_Max_minsup2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.03s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 done [35.79s].\n",
      "filtering for closed item sets ... done [0.47s].\n",
      "writing A_AP_Closed_minsup2000.txt ... [519902 set(s)] done [0.10s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -tc -s-2000 accidents_10k.dat A_AP_Closed_minsup2000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Frequent itemsets has the highest number of itemsets(851034) and maximal itemsets has lowest number of itemsets(12330). We know that frequent itemset is superset of closed itemset, which in turn is the superset of maximal itemset. So, is the reason that frequent itemsets are the most and maximal itemsets are less in number.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2e:** </span> For a minsup = 2000, compare the execution time for Apriori, ECLAT and FPGrowth. Which of these algorithms took the least amount of time. Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [17.02s].\n",
      "writing A_AP_Frequent_minsup2001.txt ... [851034 set(s)] done [0.10s].\n",
      "17 secs  610440 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-2000 accidents_10k.dat A_AP_Frequent_minsup2001.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+x eclat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ./eclat [options] infile [outfile]\n",
      "find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "-t#      target type                              (default: s)\n",
      "         (s: frequent, c: closed, m: maximal item sets,\n",
      "          g: generators, r: association rules)\n",
      "-m#      minimum number of items per set/rule     (default: 1)\n",
      "-n#      maximum number of items per set/rule     (default: no limit)\n",
      "-s#      minimum support of an item set/rule      (default: 10%)\n",
      "-S#      maximum support of an item set/rule      (default: 100%)\n",
      "         (positive: percentage, negative: absolute number)\n",
      "-o       use original rule support definition     (body & head)\n",
      "-c#      minimum confidence of an assoc. rule     (default: 80%)\n",
      "-e#      additional evaluation measure            (default: none)\n",
      "-a#      aggregation mode for evaluation measure  (default: none)\n",
      "-d#      threshold for add. evaluation measure    (default: 10%)\n",
      "-i       invalidate eval. below expected support  (default: evaluate all)\n",
      "-p#      (min. size for) pruning with evaluation  (default: no pruning)\n",
      "         (< 0: weak forward, > 0 strong forward, = 0: backward pruning)\n",
      "-q#      sort items w.r.t. their frequency        (default: 2)\n",
      "         (1: ascending, -1: descending, 0: do not sort,\n",
      "          2: ascending, -2: descending w.r.t. transaction size sum)\n",
      "-A#      variant of the eclat algorithm to use    (default: 'a')\n",
      "-x       do not prune with perfect extensions     (default: prune)\n",
      "-l#      number of items for k-items machine      (default: 16)\n",
      "         (only for algorithm variants i,r,o,   options -Ai/-Ar/-Ao)\n",
      "-j       do not sort items w.r.t. cond. support   (default: sort)\n",
      "         (only for algorithm variants i,b,t,d, options -Ai/-Ab/-At/-Ad)\n",
      "-y#      check extensions for closed/maximal sets (default: repository)\n",
      "         (0: horizontal, > 0: vertical representation)\n",
      "         (only with improved tid lists variant, option -Ai)\n",
      "-u       do not use head union tail (hut) pruning (default: use hut)\n",
      "         (only for maximal item sets, option -tm, not with option -Ab)\n",
      "-F#:#..  support border for filtering item sets   (default: none)\n",
      "         (list of minimum support values, one per item set size,\n",
      "         starting at the minimum size, as given with option -m#)\n",
      "-R#      read item selection/appearance indicators\n",
      "-P#      write a pattern spectrum to a file\n",
      "-Z       print item set statistics (number of item sets per size)\n",
      "-N       do not pre-format some integer numbers   (default: do)\n",
      "-g       write output in scanable form (quote certain characters)\n",
      "-h#      record header  for output                (default: \"\")\n",
      "-k#      item separator for output                (default: \" \")\n",
      "-I#      implication sign for association rules   (default: \" <- \")\n",
      "-v#      output format for item set information   (default: \" (%S)\")\n",
      "-w       transaction weight in last field         (default: only items)\n",
      "-r#      record/transaction separators            (default: \"\\n\")\n",
      "-f#      field /item        separators            (default: \" \\t,\")\n",
      "-b#      blank   characters                       (default: \" \\t\\r\")\n",
      "-C#      comment characters                       (default: \"#\")\n",
      "-T#      file to write transaction identifiers to (default: none)\n",
      "-!       print additional option information\n",
      "infile   file to read transactions from           [required]\n",
      "outfile  file to write item sets/assoc.rules to   [optional]\n"
     ]
    }
   ],
   "source": [
    "!./eclat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "writing A_EC_Freq_minsup2000.txt ... [851034 set(s)] done [0.34s].\n",
      "0 secs  842709 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./eclat -ts -s-2000 accidents_10k.dat A_EC_Freq_minsup2000.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+x fpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ./fpgrowth [options] infile [outfile]\r\n",
      "find frequent item sets with the fpgrowth algorithm\r\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\r\n",
      "-t#      target type                              (default: s)\r\n",
      "         (s: frequent, c: closed, m: maximal item sets,\r\n",
      "          g: generators, r: association rules)\r\n",
      "-m#      minimum number of items per set/rule     (default: 1)\r\n",
      "-n#      maximum number of items per set/rule     (default: no limit)\r\n",
      "-s#      minimum support of an item set/rule      (default: 10%)\r\n",
      "-S#      maximum support of an item set/rule      (default: 100%)\r\n",
      "         (positive: percentage, negative: absolute number)\r\n",
      "-o       use original rule support definition     (body & head)\r\n",
      "-c#      minimum confidence of an assoc. rule     (default: 80%)\r\n",
      "-e#      additional evaluation measure            (default: none)\r\n",
      "-a#      aggregation mode for evaluation measure  (default: none)\r\n",
      "-d#      threshold for add. evaluation measure    (default: 10%)\r\n",
      "-i       invalidate eval. below expected support  (default: evaluate all)\r\n",
      "-p#      (min. size for) pruning with evaluation  (default: no pruning)\r\n",
      "         (< 0: weak forward, > 0 strong forward, = 0: backward pruning)\r\n",
      "-q#      sort items w.r.t. their frequency        (default: 2)\r\n",
      "         (1: ascending, -1: descending, 0: do not sort,\r\n",
      "          2: ascending, -2: descending w.r.t. transaction size sum)\r\n",
      "-A#      variant of the fpgrowth algorithm to use (default: c)\r\n",
      "-x       do not prune with perfect extensions     (default: prune)\r\n",
      "-l#      number of items for k-items machine      (default: 16)\r\n",
      "         (only for variants s and d, options -As or -Ad)\r\n",
      "-j       do not sort items w.r.t. cond. support   (default: sort)\r\n",
      "         (only for algorithm variant c, option -Ac)\r\n",
      "-u       do not use head union tail (hut) pruning (default: use hut)\r\n",
      "         (only for maximal item sets, option -tm)\r\n",
      "-F#:#..  support border for filtering item sets   (default: none)\r\n",
      "         (list of minimum support values, one per item set size,\r\n",
      "         starting at the minimum size, as given with option -m#)\r\n",
      "-R#      read item selection/appearance indicators\r\n",
      "-P#      write a pattern spectrum to a file\r\n",
      "-Z       print item set statistics (number of item sets per size)\r\n",
      "-N       do not pre-format some integer numbers   (default: do)\r\n",
      "-g       write item names in scanable form (quote certain characters)\r\n",
      "-h#      record header  for output                (default: \"\")\r\n",
      "-k#      item separator for output                (default: \" \")\r\n",
      "-I#      implication sign for association rules   (default: \" <- \")\r\n",
      "-v#      output format for set/rule information   (default: \" (%S)\")\r\n",
      "-w       integer transaction weight in last field (default: only items)\r\n",
      "-r#      record/transaction separators            (default: \"\\n\")\r\n",
      "-f#      field /item        separators            (default: \" \\t,\")\r\n",
      "-b#      blank   characters                       (default: \" \\t\\r\")\r\n",
      "-C#      comment characters                       (default: \"#\")\r\n",
      "-!       print additional option information\r\n",
      "infile   file to read transactions from           [required]\r\n",
      "outfile  file to write item sets/assoc. rules to  [optional]\r\n"
     ]
    }
   ],
   "source": [
    "!./fpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.03s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "writing A_FP_Freq_minsup2000.txt ... [851034 set(s)] done [0.15s].\n",
      "0 secs  644124 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./fpgrowth -ts -s-2000 accidents_10k.dat A_FP_Freq_minsup2000.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** FP growth algorithm takes the least amount of time to run as it is based on projection based approach to calculate the frequent itemsets. It scans the database only once and works on prefix-span based algorithm so size of projecttion tree shrinks gradually. </span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2f:** </span> For a minsup = 4000, compare the execution time for Apriori, ECLAT and FPGrowth. Which of these algorithms took the least amount of time. Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [22267 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [1.23s].\n",
      "writing A_AP_Frequent_minsup4001.txt ... [29501 set(s)] done [0.00s].\n",
      "1 secs  524052 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-4000 accidents_10k.dat A_AP_Frequent_minsup4001.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.01s].\n",
      "writing A_EC_Freq_minsup4000.txt ... [29501 set(s)] done [0.04s].\n",
      "0 secs  314034 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./eclat -ts -s-4000 accidents_10k.dat A_EC_Freq_minsup4000.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.00s].\n",
      "writing A_FP_Freq_minsup4000.txt ... [29501 set(s)] done [0.02s].\n",
      "0 secs  294505 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./fpgrowth -ts -s-4000 accidents_10k.dat A_FP_Freq_minsup4000.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** FP growth algorithm takes the least amount of time to run as it is based on projection based approach to calculate the frequent itemsets. It scans the database only once and works on prefix-span based algorithm so size of projecttion tree shrinks gradually.  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2g:** </span>  For a minsup = 6000, compare the execution time for Apriori, ECLAT and FPGrowth. Which of these algorithms took the least amount of time. Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [6478 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 done [0.04s].\n",
      "writing A_AP_Frequent_minsup6001.txt ... [2254 set(s)] done [0.00s].\n",
      "0 secs  276758 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-6000 accidents_10k.dat A_AP_Frequent_minsup6001.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.00s].\n",
      "writing A_EC_Freq_minsup6000.txt ... [2254 set(s)] done [0.01s].\n",
      "0 secs  284987 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./eclat -ts -s-6000 accidents_10k.dat A_EC_Freq_minsup6000.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.00s].\n",
      "writing A_FP_Freq_minsup6000.txt ... [2254 set(s)] done [0.01s].\n",
      "0 secs  288566 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./fpgrowth -ts -s-6000 accidents_10k.dat A_FP_Freq_minsup6000.txt\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** FP growth algorithm takes the least amount of time to run as it is based on projection based approach to calculate the frequent itemsets. It scans the database only once and works on prefix-span based algorithm so size of projecttion tree shrinks gradually.  </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2h:** </span> Fill the following table based on execution times computed in __2e__, __2f__, and __2g__. State your observations on the relative computational efficiency at different support thresholds. Based on your knowledge of these algorithms, provide the reasons behind your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Algorithm                |minsup=2000             |minsup=4000            |minsup=6000            |\n",
    "|----------------------------|------------------------|-----------------------|-----------------------|    \n",
    "|Apriori                     |22 secs 461071 microsecs|0 secs 844267 microsecs|0 secs 760493 microsecs|\n",
    "|Eclat                       |2 secs 815011 microsecs |0 secs 346730 microsecs|0 secs 336399 microsecs|\n",
    "|FPGrowth                    |0 secs 306495 microsecs |0 secs 276695 microsecs|0 secs 271429 microsecs|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** It's noticed that the execution times of FP growth takes least amount of time beacuse it finds the frequent itemsets in the database without candiate generation and it follows prefix-span algorithm, so it takes less support computation time. In Eclat ,we can see it improves the support computation but not better when compared to the Fp growth as it is scanning dataset at each level. In Apriori , we can see the computation time is higher because it does candidate generation and support computation for each itemset takes the whole database scan, so it takes the most computation time.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discovering frequent subsequences and substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that roads in a Cincinnati are assigned numbers. Participants are enrolled in a transportation study and for every trip they make using their car, the sequence of roads taken are recorded. Trips that involves freeways are excluded. This data is in the file <span style=\"color:blue\">road_seq_data.dat</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3a:** </span>  What 'type' of sequence mining will you perform to determine frequently taken 'paths'? Paths are sequences of roads traveresed consecutively in the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Substring mining bcuz order </span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3b:** </span> How many sequences are there in this sequence database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+x prefixspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrefixSpan version 1.00 - Sequential Pattern Miner\r\n",
      "Written by Yasuo Tabei\r\n",
      "\r\n",
      "\r\n",
      "Usage: prefixspan [OPTION]... INFILE\r\n",
      "\r\n",
      "       where [OPTION]...  is a list of zero or more optional arguments\r\n",
      "             INFILE(s)    is the name of the input transaction database\r\n",
      "\r\n",
      "Additional arguments (at most one input file may be specified):\r\n",
      "       -min_sup [minimum support]\r\n",
      "       -max_pat [maximum pattern]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./prefixspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 road_seq_data.dat\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l road_seq_data.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** There are 1000 sequences in road_seq_data.dat file. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3c:** </span> What is the size of the alphabet in this sequence database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\r\n"
     ]
    }
   ],
   "source": [
    "!awk -- '{for (i = 1; i <= NF; i++) wc[$i] += 1}; END {print length(wc)}' road_seq_data.dat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** The size of alphabet is 1283 in the road_seq_data.dat database. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3d:** </span> What are the total number of possible subsequences of length 2 in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrefixSpan version 1.00 - Sequential Pattern Miner\r\n",
      "Written by Yasuo Tabei\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./prefixspan -min_sup 1 road_seq_data.dat | sed -n 'p;n' > subsequence.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15927\r\n"
     ]
    }
   ],
   "source": [
    "!awk 'NF==2 { count++} END {print count}' subsequence.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3e:** </span> What are the total number of possible substrings of length 2 in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+x seqwog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ./seqwog [options] infile [outfile]\r\n",
      "find frequent sequences without gaps\r\n",
      "version 3.16 (2016.10.15)        (c) 2010-2016   Christian Borgelt\r\n",
      "-t#      target type                              (default: s)\r\n",
      "         (s: frequent, c: closed, m: maximal sequences, r: rules)\r\n",
      "         (target type 'r' implies -a (all occurrences))\r\n",
      "-m#      minimum number of items per sequence     (default: 1)\r\n",
      "-n#      maximum number of items per sequence     (default: no limit)\r\n",
      "-s#      minimum support of a sequence            (default: 10%)\r\n",
      "         (positive: percentage, negative: absolute number)\r\n",
      "-o       use original rule support definition     (body & head)\r\n",
      "-c#      minimum confidence of a     rule         (default: 80%)\r\n",
      "-a       count all occurrences of a pattern       (default: #sequences)\r\n",
      "-F#:#..  support border for filtering item sets   (default: none)\r\n",
      "         (list of minimum support values, one per item set size,\r\n",
      "         starting at the minimum size, as given with option -m#)\r\n",
      "-P#      write pattern spectrum to a file\r\n",
      "-Z       print item set statistics (number of item sets per size)\r\n",
      "-g       write output in scanable form (quote certain characters)\r\n",
      "-h#      record header  for output                (default: \"\")\r\n",
      "-k#      item separator for output                (default: \" \")\r\n",
      "-I#      implication sign for sequence rules      (default: \" -> \")\r\n",
      "-v#      output format for sequence information   (default: \" (%S)\")\r\n",
      "-w       integer transaction weight in last field (default: only items)\r\n",
      "-r#      record/transaction separators            (default: \"\\n\")\r\n",
      "-f#      field /item        separators            (default: \" \\t,\")\r\n",
      "-b#      blank   characters                       (default: \" \\t\\r\")\r\n",
      "-C#      comment characters                       (default: \"#\")\r\n",
      "-!       print additional option information\r\n",
      "infile   file to read transactions from           [required]\r\n",
      "outfile  file to write frequent sequences to      [optional]\r\n"
     ]
    }
   ],
   "source": [
    "!./seqwog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./seqwog - find frequent sequences without gaps\r\n",
      "version 3.16 (2016.10.15)        (c) 2010-2016   Christian Borgelt\r\n",
      "reading road_seq_data.dat ... [1283 item(s), 1000 transaction(s)] done [0.00s].\r\n",
      "recoding items ... [1283 item(s)] done [0.00s].\r\n",
      "reducing and triming transactions ... [883/1000 transaction(s)] done [0.00s].\r\n",
      "writing substring_result ... [1390 sequence(s)] done [0.01s].\r\n"
     ]
    }
   ],
   "source": [
    "!./seqwog -ts -s-2 -n2 road_seq_data.dat substring_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1390 substring_result\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l substring_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3f:** </span> Discover frequent __subsequences__ with minsup = 10 and report the number of subsequences discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrefixSpan version 1.00 - Sequential Pattern Miner\r\n",
      "Written by Yasuo Tabei\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./prefixspan -min_sup 10 road_seq_data.dat | sed -n 'p;n' > roadseq_Freq_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4589 roadseq_Freq_10\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l roadseq_Freq_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** There are 4589 frequent subsequences for minsup = 10. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3g:** </span>  Discover frequent __substrings__ with minsup = 10 and report the number of substrings discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./seqwog - find frequent sequences without gaps\r\n",
      "version 3.16 (2016.10.15)        (c) 2010-2016   Christian Borgelt\r\n",
      "reading road_seq_data.dat ... [1283 item(s), 1000 transaction(s)] done [0.00s].\r\n",
      "recoding items ... [1283 item(s)] done [0.00s].\r\n",
      "reducing and triming transactions ... [844/1000 transaction(s)] done [0.00s].\r\n",
      "writing substring_result_10 ... [613 sequence(s)] done [0.00s].\r\n"
     ]
    }
   ],
   "source": [
    "!./seqwog -ts -s-10 road_seq_data.dat substring_result_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613 substring_result_10\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l substring_result_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** There are 613 frequent substrings with minsup = 10.</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3h:** </span> Explain the difference in the number of frequent subsequences and substrings found in __3f__ and __3g__ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** There is a huge difference in the subsequence and substrings because the substrings will not allow gaps to be considered from a particular sequence, but subsequence are consecutive in form and they eliminate multiple subsequences. </span> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 [python/3.5]",
   "language": "python",
   "name": "sys_python35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
